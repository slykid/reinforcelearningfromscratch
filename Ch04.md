## 0. Review
* 강화학습에서는 보편적으로 아래 2가지 유형의 문제를 해결하며, 최종적으로는 정책제어를 통해 최적의 정책을 만드는 것이 목표입니다.
	* 정책평가: 정책 $\pi$가 주어졌을 때 그 정책의 가치함수 $v_{\pi}{(s)}$ 또는 $q_{\pi}{(s, a)}$를 구하는 문제
  * 정책제어: 정책을 조정하여 최적 정책을 만드는 문제

* 벨만 방정식을 통해 연립 방정식을 얻어, 에이전트의 상태 전이 확률, 보상함수, 정책 정보를 계산할 수 있습니다.
  * $v_{\pi}{(s)} = \mathbb{E} $
  
* 하지만, 상태와 행동 패턴 수가 조금만 많아져도 계산이 복잡하다는 단점이 존재했으며, 이를 해결하기 위해 등장한 것이 **"동적 프로그래밍"** 기법입니다.

## 4.1 동적 프로그래밍 기초
---
* 
