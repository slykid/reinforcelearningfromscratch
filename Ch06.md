# Chapter 6. TDë²•
TD(Temporal Difference) : í™˜ê²½ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , í–‰ë™ì„ í•œ ë²ˆ ìˆ˜í–‰í•  ë•Œë§ˆë‹¤ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ê°±ì‹ 

## 6.1 TDë²•ìœ¼ë¡œ ì •ì±… í‰ê°€í•˜ê¸°

**TDë²•**: **ëª¬í…Œì¹´ë¥¼ë¡œë²•**ê³¼ **ë™ì  í”„ë¡œê·¸ë˜ë°**ì„ í•©ì¹œ ê¸°ë²•
* ë™ì  í”„ë¡œê·¸ë˜ë°ì²˜ëŸ¼ ë¶€íŠ¸ìŠ¤íŠ¸ë©ì„ í†µí•´ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê°±ì‹ 
* ëª¬í…Œì¹´ë¥¼ë¡œë²•ì²˜ëŸ¼ í™˜ê²½ì— ëŒ€í•œ ì •ë³´ ì—†ì´ ìƒ˜í”Œë§ëœ ë°ì´í„°ë§Œìœ¼ë¡œ ê°€ì¹˜ í•¨ìˆ˜ ê°±ì‹ 

**TDë²• ê°±ì‹ ì‹**:

$$V_\pi'(S_t)=V_\pi(S_t)+\alpha\\{R_t+\gamma V_\pi(S_{t+1})-V_\pi(S_t)\\}$$

$V_\pi$ : ê°€ì¹˜ í•¨ìˆ˜ì˜ ì¶”ì •ì¹˜

$R_t+\gamma V_\pi(S_{t+1})$ : TD ëª©í‘œ <br><sub>ì¶”ì •ì¹˜ë¡œ ì¶”ì •ì¹˜ë¥¼ ê°±ì‹ í•˜ëŠ” <i>ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘</i>ìœ¼ë¡œ, ì¶”ì •ì¹˜ë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì— í¸í–¥ì´ ìˆìŒ</sub>

$V_\pi(S_t)$ ë¥¼ TD ëª©í‘œ ë°©í–¥ìœ¼ë¡œ ê°±ì‹ 

```python
from collections import defaultdict
import numpy as np
from common.gridworld import GridWorld


class TdAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.01
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)
        self.V = defaultdict(lambda: 0)

    def get_action(self, state):
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def eval(self, state, reward, next_state, done):
        next_V = 0 if done else self.V[next_state]  # ëª©í‘œ ì§€ì ì˜ ê°€ì¹˜ í•¨ìˆ˜ëŠ” 0
        target = reward + self.gamma * next_V
        self.V[state] += (target - self.V[state]) * self.alpha


env = GridWorld()
agent = TdAgent()

episodes = 1000
for episode in range(episodes):
    state = env.reset()

    while True:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)

        agent.eval(state, reward, next_state, done)  # ë§¤ë²ˆ í˜¸ì¶œ
        if done:
            break
        state = next_state

env.render_v(agent.V)
```
---

## 6.2 SARSA (ì˜¨-ì •ì±…)

**Qí•¨ìˆ˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ TDë²• ê°±ì‹ ì‹**:

$$Q_\pi'(S_t, A_t)=Q_\pi(S_t, A_t)+\alpha\\{R_t+\gamma Q_\pi(S_{t+1}, A_{t+1})-Q_\pi(S_t, A_t)\\}$$

ì˜¨-ì •ì±…ì—ì„œ ì—ì´ì „íŠ¸ëŠ” ì •ì±…ì„ í•˜ë‚˜ë§Œ ê°€ì§€ê³  ìˆìŒ(í–‰ë™ ì •ì±… = ëŒ€ìƒ ì •ì±…)

```python
from collections import defaultdict, deque
import numpy as np
from common.gridworld import GridWorld
from common.utils import greedy_probs


class SarsaAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)
        self.Q = defaultdict(lambda: 0)
        self.memory = deque(maxlen=2)  # deque ì‚¬ìš©

    def get_action(self, state):
        action_probs = self.pi[state]  # piì—ì„œ ì„ íƒ
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def reset(self):
        self.memory.clear()

    def update(self, state, action, reward, done):
        self.memory.append((state, action, reward, done))
        if len(self.memory) < 2:
            return

        state, action, reward, done = self.memory[0]
        next_state, next_action, _, _ = self.memory[1]
        next_q = 0 if done else self.Q[next_state, next_action]  # ë‹¤ìŒ Q í•¨ìˆ˜

        # TDë²•ìœ¼ë¡œ self.Q ê°±ì‹ 
        target = reward + self.gamma * next_q
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha
        
        # ì •ì±… ê°œì„ 
        self.pi[state] = greedy_probs(self.Q, state, self.epsilon)


env = GridWorld()
agent = SarsaAgent()

episodes = 10000
for episode in range(episodes):
    state = env.reset()
    agent.reset()

    while True:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)

        agent.update(state, action, reward, done)  # ë§¤ë²ˆ í˜¸ì¶œ

        if done:
            # ëª©í‘œì— ë„ë‹¬í–ˆì„ ë•Œë„ í˜¸ì¶œ
            agent.update(next_state, None, None, None)
            break
        state = next_state

env.render_q(agent.Q)
```
---

## 6.3 ì˜¤í”„-ì •ì±… SARSA

**ì˜¤í”„-ì •ì±… SARSA ê°±ì‹ ì‹**:

$$ìƒ˜í”Œë§: A_{t+1}\sim\mathit{b}$$

$$\rho =\frac{\pi(A_{t+1} \mid S_{t+1})}{\mathit{b}(A_{t+1} \mid S_{t+1})}$$

$$Q_\pi'(S_t, A_t)=Q_\pi(S_t, A_t)+\alpha\\{\rho(R_t+\gamma Q_\pi(S_{t+1}, A_{t+1}))-Q_\pi(S_t, A_t)\\}$$

ì˜¤í”„-ì •ì±…ì—ì„œëŠ” ì—ì´ì „íŠ¸ê°€ í–‰ë™ ì •ì±…ê³¼ ëŒ€ìƒ ì •ì±…ì„ ë”°ë¡œ ê°€ì§€ê³  ìˆìŒ

ë‘ ì •ì±…ì´ ì—­í• ì„ ë¶„ë‹´í•˜ì—¬ í–‰ë™ ì •ì±…ìœ¼ë¡œëŠ” 'íƒìƒ‰'ì„, ëŒ€ìƒ ì •ì±…ìœ¼ë¡œëŠ” 'í™œìš©'ì„ ìˆ˜í–‰

ê°€ì¤‘ì¹˜ ÏëŠ” 'ì •ì±…ì´ Ï€ì¼ ë•Œ TD ëª©í‘œë¥¼ ì–»ì„ í™•ë¥ ' ê³¼ 'ì •ì±…ì´ ğ‘ì¼ ë•Œ TD ëª©í‘œë¥¼ ì–»ì„ í™•ë¥ 'ì˜ ë¹„ìœ¨

```python
from collections import defaultdict, deque
import numpy as np
from common.gridworld import GridWorld
from common.utils import greedy_probs


class SarsaOffPolicyAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)
        self.b = defaultdict(lambda: random_actions)
        self.Q = defaultdict(lambda: 0)
        self.memory = deque(maxlen=2)

    def get_action(self, state):
        action_probs = self.b[state]  # í–‰ë™ ì •ì±…ì—ì„œ ê°€ì ¸ì˜´
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def reset(self):
        self.memory.clear()

    def update(self, state, action, reward, done):
        self.memory.append((state, action, reward, done))
        if len(self.memory) < 2:
            return

        state, action, reward, done = self.memory[0]
        next_state, next_action, _, _ = self.memory[1]

        if done:
            next_q = 0
            rho = 1
        else:
            next_q = self.Q[next_state, next_action]
            rho = self.pi[next_state][next_action] / self.b[next_state][next_action]  # ê°€ì¤‘ì¹˜ rho ê³„ì‚°

        # rhoë¡œ TD ëª©í‘œ ë³´ì •
        target = rho * (reward + self.gamma * next_q)
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha

        # ê°ê°ì˜ ì •ì±… ê°œì„ 
        self.pi[state] = greedy_probs(self.Q, state, 0)
        self.b[state] = greedy_probs(self.Q, state, self.epsilon)


env = GridWorld()
agent = SarsaOffPolicyAgent()

episodes = 10000
for episode in range(episodes):
    state = env.reset()
    agent.reset()

    while True:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)

        agent.update(state, action, reward, done)

        if done:
            agent.update(next_state, None, None, None)
            break
        state = next_state

env.render_q(agent.Q)
```
---

## 6.4 Q ëŸ¬ë‹

### Q ëŸ¬ë‹ ëŒ€í‘œ íŠ¹ì§•
* TD ë²•
* ì˜¤í”„-ì •ì±…
* ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

ë‘ ì •ì±…ì˜ í™•ë¥  ë¶„í¬ê°€ ë‹¤ë¥¼ ìˆ˜ë¡ ì¤‘ìš”ë„ ìƒ˜í”Œë§ì˜ ê°€ì¤‘ì¹˜ Ïì˜ ë³€ë™ì„±ì´ ì»¤ì ¸ ê²°ê³¼ê°€ ë¶ˆì•ˆì •í•˜ê¸° ë•Œë¬¸ì— Q ëŸ¬ë‹ì—ì„œëŠ” ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

**Q ëŸ¬ë‹ ê°±ì‹ ì‹**:

$$Q'(S_t, A_t)=Q(S_t, A_t)+\alpha\\{R_t+\gamma \max_a Q(S_{t+1}, a)-Q(S_t, A_t)\\}$$

Q í•¨ìˆ˜ê°€ ê°€ì¥ í° í–‰ë™ìœ¼ë¡œ $A_{t+1}$ì„ ì„ íƒ

```python
from collections import defaultdict
import numpy as np
from common.gridworld import GridWorld
from common.utils import greedy_probs


class QLearningAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)  # ëŒ€ìƒ ì •ì±…
        self.b = defaultdict(lambda: random_actions)  # í–‰ë™ ì •ì±…
        self.Q = defaultdict(lambda: 0)

    def get_action(self, state):
        action_probs = self.b[state]  # í–‰ë™ ì •ì±…ì—ì„œ ê°€ì ¸ì˜´
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def update(self, state, action, reward, next_state, done):
        if done:  # ëª©í‘œì— ë„ë‹¬
            next_q_max = 0
        else:     # ê·¸ ì™¸ì—ëŠ” ë‹¤ìŒ ìƒíƒœì—ì„œ Q í•¨ìˆ˜ì˜ ìµœëŒ“ê°’ ê³„ì‚°
            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]
            next_q_max = max(next_qs)

        # Q í•¨ìˆ˜ ê°±ì‹ 
        target = reward + self.gamma * next_q_max
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha

        # í–‰ë™ ì •ì±…ê³¼ ëŒ€ìƒ ì •ì±… ê°±ì‹ 
        self.pi[state] = greedy_probs(self.Q, state, epsilon=0)  # greedy
        self.b[state] = greedy_probs(self.Q, state, self.epsilon)  # epsilon-greedy


env = GridWorld()
agent = QLearningAgent()

episodes = 10000
for episode in range(episodes):
    state = env.reset()

    while True:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)

        agent.update(state, action, reward, next_state, done)
        if done:
            break
        state = next_state

env.render_q(agent.Q)
```
---

## 6.5 ë¶„í¬ ëª¨ë¸ê³¼ ìƒ˜í”Œ ëª¨ë¸

6.4 ì—ì„œëŠ” ì—ì´ì „íŠ¸ë¥¼ ë¶„í¬ ëª¨ë¸ë¡œ êµ¬í˜„

í™•ë¥  ë¶„í¬ë¥¼ ìœ ì§€í•˜ì§€ ì•Šìœ¼ë©´(ì •ì±…ì„ ìœ ì§€í•˜ì§€ ì•ŠìŒ) ë³´ë‹¤ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„ ê°€ëŠ¥

```python
from collections import defaultdict
import numpy as np
from common.gridworld import GridWorld

class QLearningAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4
        self.Q = defaultdict(lambda: 0)

    def get_action(self, state):
        if np.random.rand() < self.epsilon:  # epsilonì˜ í™•ë¥ ë¡œ ë¬´ì‘ìœ„ í–‰ë™
            return np.random.choice(self.action_size)
        else:                                # (1 - epsilon)ì˜ í™•ë¥ ë¡œ íƒìš• í–‰ë™
            qs = [self.Q[state, a] for a in range(self.action_size)]
            return np.argmax(qs)

    def update(self, state, action, reward, next_state, done):
        if done:
            next_q_max = 0
        else:
            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]
            next_q_max = max(next_qs)

        target = reward + self.gamma * next_q_max
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha


env = GridWorld()
agent = QLearningAgent()

episodes = 1000
for episode in range(episodes):
    state = env.reset()

    while True:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)

        agent.update(state, action, reward, next_state, done)
        if done:
            break
        state = next_state

env.render_q(agent.Q)
```
---

## 6.6 ì •ë¦¬

1. TDë²•ì€ ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ í–‰ë™í•œ ê²°ê³¼ë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ í‰ê°€

2. 'ì§€ê¸ˆ' ê³¼ 'ë‹¤ìŒ' ì˜ ì •ë³´ë§Œìœ¼ë¡œ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ê°±ì‹ 

3. ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë” ë¹ ë¥´ê²Œ ê°±ì‹ í•  ìˆ˜ ìˆìŒ

4. **SARSA** : ì˜¨-ì •ì±… ë°©ì‹. TDë²•ìœ¼ë¡œ Qí•¨ìˆ˜ë¥¼ í‰ê°€í•˜ê³  Îµ-íƒìš•í™”ë¥¼ í†µí•´ ì •ì±… ê°œì„ 

5. Îµ-íƒìš•í™”ë¥¼ í†µí•´ ì •ì±…ì—ì„œ 'íƒìƒ‰'ê³¼ 'í™œìš©'ì˜ ê· í˜•ì„ ë§ì¶œ ìˆ˜ ìˆìŒ

6. **Q ëŸ¬ë‹** : ì˜¤í”„-ì •ì±… ë°©ì‹. ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ì„œ Q í•¨ìˆ˜ ê°±ì‹ 
