# Chapter 5. 몬테카를로법

## 5.1 몬테카를로법 기초

**개념**: 환경 모델($P, R$)을 모르는 상황에서 에이전트의 **직접적인 경험**을 통해 가치 함수를 추정하는 모델-프리(Model-Free) 학습 방식입니다.

**원리**: 큰 수의 법칙을 기반으로, 충분히 많은 에피소드를 경험하면 반환값($G_t$)의 평균이 실제 가치 함수 값에 수렴합니다. 즉, 경험을 통해 얻은 샘플 데이터의 평균이 수학적 기댓값과 같아집니다.

### 증분 방식의 평균 계산

샘플 데이터를 얻을 때마다 평균을 구해야 할 때는 **증분 방식**이 더 효율적입니다.

**장점**:
- 모든 샘플을 저장할 필요 없음 (메모리 효율적)
- 새로운 샘플이 들어올 때마다 즉시 업데이트 가능
- 수치적으로 안정적

**증분 방식 공식**:

$$V_n = V_{n-1} + \frac{1}{n}(s_n - V_{n-1})$$

```python
import numpy as np
# 주사위를 2개 던져 합의 평균 
def sample(dices=2):
    x = 0
    for _ in range(dices):
        x += np.random.choice([1, 2, 3, 4, 5, 6])
    return x

trial = 1000
V, n = 0, 0

for _ in range(trial):
    s = sample()
    n += 1
    V += (s - V) / n  # 증분 방식으로 평균 갱신
    print(V)
```

---

## 5.2 몬테카를로법으로 정책 평가하기

상태 $s$의 가치 함수 $V_{\pi}(s)$를 $N$번의 에피소드 경험을 통해 얻은 반환값 $G$들의 평균으로 추정하는 몬테카를로법의 수식은 다음과 같습니다.

$$V_{\pi}(s) \approx \frac{G^{(1)} + G^{(2)} + \cdots + G^{(N)}}{N}$$

**수식 설명**:
- $V_{\pi}(s)$: 정책 $\pi$를 따를 때 상태 $s$의 가치
- $G^{(i)}$: $i$번째 에피소드에서 상태 $s$를 방문한 후 얻은 반환값
- $N$: 총 에피소드 수

이는 실제 경험을 통해 얻은 수익들의 평균이 그 상태의 미래 기대 수익이 된다는 **큰 수의 법칙 원리**를 나타냅니다.

### 반환값($G_t$)의 계산

특정 시점 $t$ 이후부터 에피소드가 끝날 때까지 얻는 **할인된 보상($R$)의 총합**입니다.

**반환값 $G_t$**는 다음 공식으로 계산됩니다:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots$$

여기서 $\gamma$는 할인율(Discount Factor)로, 미래 보상의 현재 가치를 조절합니다.

---

## 5.3 몬테카를로법 구현

### 몬테카를로 정책 평가 알고리즘

**핵심 흐름**:
1. 에이전트가 정책에 따라 행동하며 경험 수집 (`add`)
2. 에피소드 종료 후 역순으로 반환값($G_t$) 계산
3. 증분 방식으로 가치 함수 갱신 (`eval`)

### 에이전트 클래스 구현

에이전트가 행동을 선택하고 경험을 통해 학습하는 것이 핵심입니다.

```python
from collections import defaultdict
import numpy as np

class RandomAgent:
    def __init__(self):
        # --- 몬테카를로법 관련 기본 설정 ---
        self.gamma = 0.9                     # 할인율: 미래 보상의 가치를 현재로 환산하는 비율
        self.action_size = 4                 # 환경에서 가능한 행동의 개수 (예: 상, 하, 좌, 우)

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25} 
        # 정책(π): 무작위 행동을 할 확률 분포
        self.pi = defaultdict(lambda: random_actions) 
        
        # 상태 가치 함수(V(s)): 각 상태의 기대 수익을 저장. 초기값은 0
        self.V = defaultdict(lambda: 0)
        
        # 증분방식으로 수익의 평균을 구할 때 사용
        self.cnts = defaultdict(lambda: 0) 
        
        # 에이전트가 하나의 에피소드 동안 얻은 경험 (상태, 행동, 보상)을 저장
        self.memory = []

    def get_action(self, state):
        #현재 정책(self.pi)에 따라 행동을 선택
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        # 확률(p=probs)에 기반하여 행동을 무작위로 샘플링하여 반환
        return np.random.choice(actions, p=probs)

    def add(self, state, action, reward):
        # 얻은 경험을 메모리에 추가
        # 목표지점의 가치함수는 항상 0이기 때문에 마지막 상태는 저장되지 않는 점을 유의
        data = (state, action, reward)
        self.memory.append(data)

    def reset(self):
        #하나의 에피소드가 끝난 후, 다음 에피소드를 위해 메모리를 초기화
        self.memory.clear()

    def eval(self):
        G = 0  # 반환값(Return, Gt)을 계산하기 위한 초기값 설정
        
        # 1. 에피소드 경험을 역순으로 순회하며 각 상태에서 얻은 수익을 계산
        for data in reversed(self.memory):
            state, action, reward = data
            
            # Gt = Rt+1 + gamma * Gt+1 공식을 사용하여 G를 갱신
            G = self.gamma * G + reward
            
            # 2. 가치 함수(V(s))를 갱신
            
            # 해당 상태의 방문 횟수를 1 증가 (N(s) 갱신)
            self.cnts[state] += 1
            
            # V(s) 갱신: V_new = V_old + 1/N * (G - V_old) (수익 이동 평균)
            # 이를 통해 V[state]는 G 값들의 누적 평균으로 수렴하게 됨
            self.V[state] += (G - self.V[state]) / self.cnts[state]
```

### 몬테카를로법 실행

GridWorld 환경과 연동하여 실행해보았습니다.

```python
env = GridWorld()
agent = RandomAgent()

episodes = 1000
for episode in range(episodes):
    state = env.reset()
    agent.reset()

    while True:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)

        agent.add(state, action, reward)
        if done:
            agent.eval()
            break

        state = next_state

# 가치함수 시각화
env.render_v(agent.V)
```

**실행 과정**:
1. 총 1000번의 에피소드를 실행
2. 에이전트가 먼저 행동하고 그 결과로 얻은 샘플 데이터를 기록
3. 샘플데이터를 이용하여 몬테카를로법으로 가치함수를 갱신

**핵심 결과**: 몬테카를로법을 이용하면 **환경 모델을 몰라도 정책 평가를 제대로 할 수 있습니다.**

가치함수를 시각화한 결과는 아래와 같습니다.

<img width="520" height="394" alt="image" src="https://github.com/user-attachments/assets/372c39b2-f953-4db7-9318-c93647aa26be" />

**결과 해석**: 
목표 지점(Goal)에 가까울수록 가치가 높게 학습되었으며, 무작위 정책임에도 올바른 가치 함수를 추정했습니다.

---

## 5.4 몬테카를로법으로 정책 제어하기

최적 정책은 **평가(Evaluation)**와 **개선(Improvement)**을 번갈아 반복하여 얻습니다. 
- **평가 단계**: 가치함수를 추정
- **개선 단계**: 가치함수를 탐욕화하여 정책을 개선

이 과정을 반복하여 최적 정책에 점차 가까워집니다.

### 왜 행동 가치 함수($Q$)가 필요한가?

**문제점**: 상태 가치 함수 $V(s)$만으로는 어떤 행동을 취해야 할지 알 수 없습니다.

- **모델이 있을 때**: $V(s)$를 알면 전이 확률 $P(s'|s,a)$를 사용해 최선의 행동 선택 가능
- **모델이 없을 때**: $P(s'|s,a)$를 모르므로 다음 상태를 예측할 수 없음

**해결책**: 특정 행동 $a$를 했을 때의 가치인 **행동 가치 함수 $Q(s, a)$**를 직접 추정하여 정책을 개선합니다.

### $\epsilon$-Greedy 정책

**목적**: **탐색(Exploration)**과 **활용(Exploitation)**의 균형을 유지하여 최적 정책($\pi^*$)을 찾습니다.

**동작 방식**:
- $(1-\epsilon)$ 확률: 최고 Q값을 가진 행동 선택 (활용)
- $\epsilon$ 확률: 무작위 행동 선택 (탐색)

```python
# Q 테이블을 기반으로 ε-Greedy 정책의 행동 확률을 계산하는 함수
def greedy_probs(Q, state, epsilon=0, action_size=4):
    # Q 값 추출 및 최적 행동(탐욕적 행동) 찾기

    # 현재 상태에서 각 행동의 Q 값을 리스트로 만듭니다
    qs = [Q[(state, action)] for action in range(action_size)] 
    # Q 값이 가장 큰 행동의 인덱스(max_action)를 찾습니다
    max_action = np.argmax(qs)

    # 모든 행동에 기본 확률(base_prob)을 설정합니다
    base_prob = epsilon / action_size 
    action_probs = {action: base_prob for action in range(action_size)}  
    # 최선의 행동에 (1-epsilon) 확률을 추가로 부여합니다
    action_probs[max_action] += (1 - epsilon)
    return action_probs


class McAgent:
    def __init__(self):
        # --- 몬테카를로 제어를 위한 기본 설정 ---
        self.gamma = 0.9                     # 할인율(Discount Factor, γ)
        self.epsilon = 0.1                   # ε-Greedy 탐색 비율. 정책 개선에 사용됩니다
        self.alpha = 0.1                     # 상수 학습률(α): Q 값 갱신 시 최신 경험의 반영 비율
        self.action_size = 4                 # 환경에서 가능한 행동의 개수

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25} 
        # 정책(π): 상태별 행동 확률을 저장하며, 초기에는 무작위 정책. 이후 greedy_probs로 개선됩니다
        self.pi = defaultdict(lambda: random_actions) 
        # 행동 가치 함수(Q(s, a)): 각 상태-행동 쌍의 기대 수익을 저장
        self.Q = defaultdict(lambda: 0) 
        # 메모리(Memory): 하나의 에피소드 경험 (상태, 행동, 보상)을 저장
        self.memory = []

    def get_action(self, state):
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def add(self, state, action, reward):
        data = (state, action, reward)
        self.memory.append(data)

    def reset(self):
        self.memory.clear()

    def update(self):
        G = 0  # 반환값(Return, Gt)을 계산하기 위한 초기값
        
        # 1. 에피소드 경험을 역순으로 순회하며 Gt를 계산
        for data in reversed(self.memory):
            state, action, reward = data
            
            # Gt = Rt+1 + gamma * Gt+1 재귀 공식을 사용하여 G를 계산
            G = self.gamma * G + reward
            
            # 2. Q 값 갱신 (상수 학습률 α 사용)
            key = (state, action)
            # Q_new = Q_old + α * (G - Q_old) 공식을 사용. (G - Q_old)는 오차
            # 방문 횟수(N) 대신 상수 α를 사용하므로 N 테이블이 필요 없음
            self.Q[key] += (G - self.Q[key]) * self.alpha
            
            # 3. 정책 개선 (Policy Improvement)
            # 갱신된 Q 값을 바탕으로 해당 상태의 정책(pi)을 ε-Greedy 방식으로 개선
            # 이 정책(pi)은 다음 에피소드의 행동 선택에 사용됩니다
            self.pi[state] = greedy_probs(self.Q, state, self.epsilon)
```

**참고사항**:
- 무작위성을 첨가하여 에이전트가 낮은 확률로 무작위적인 행동을 하도록 유도합니다
- 대다수 경우에 탐욕 행동을 취하기 때문에 최적 정책에도 가깝습니다
- 만약 탐욕 행동만을 수행하면 에이전트의 경로가 하나만 나올 수밖에 없습니다
- 지수이동평균 방식(상수 학습률 $\alpha$)을 활용하여 수익이라는 샘플 데이터가 생성되는 확률 분포가 시간에 따라 달라지는 것을 반영하였습니다

### RandomAgent vs McAgent 비교

| 구분 | RandomAgent (정책 평가) | McAgent (정책 제어) |
|------|------------------------|---------------------|
| 강화 학습 목표 | 주어진 정책의 가치(V)를 평가 | 최적 정책(π*)을 습득/제어 |
| 주요 추정 함수 | 상태 가치 함수 V(s) | 행동 가치 함수 Q(s, a) |
| 정책 (π) | 고정됨 (균등 무작위) | 변함 (ε-Greedy 기반으로 Q에 맞춰 개선) |
| 갱신 대상 | self.cnts[state] (상태 방문 횟수) | self.Q와 self.pi[state] (정책) |
| 갱신 메서드 | eval() (가치 평가만 수행) | update() (평가 후 정책 개선까지 수행) |

---

## 5.5 오프-정책과 중요도 샘플링

### 주요 개념

- **온-정책(On-Policy)**: 스스로 쌓은 경험을 토대로 자신의 정책을 개선하는 것
- **오프-정책(Off-Policy)**: 자신과 다른 환경에서 얻은 경험을 토대로 자신의 정책을 개선하는 것
- **대상 정책(Target Policy)**: 평가하고 개선하려는 정책
- **행동 정책(Behavior Policy)**: 에이전트가 실제로 행동을 취할 때 활용하는 정책(샘플 데이터 생성)

오프-정책이라면 행동 정책에서 **'탐색'**을, 대상 정책에서는 **'활용'**만을 할 수 있습니다.

### 중요도 샘플링(Importance Sampling)

**정의**: 어떤 확률분포의 기댓값을 다른 확률 분포에서 샘플링한 데이터를 사용하여 계산하는 기법입니다.

#### 수식 유도

**기본 기댓값 (목표)**
확률분포 $\pi$에서의 기댓값과 몬테카를로법의 근사값으로 유도합니다.

$$E_{\pi}[x] \approx \frac{1}{N} \sum_{i=1}^{N} x^{(i)}$ \approx \frac{1}{N} \sum_{i=1}^{N} x^{(i)}$$

**중요도 샘플링 변환**
$\pi$에서 샘플링하기 어렵지만, 다른 분포 $b$(행동 정책)에서는 샘플링이 쉬운 경우, 분포 $b$에서 샘플링하되, 중요도 가중치로 보정해줍니다.

$$E_{\pi}[x] = \sum_{x} \pi(x) \cdot x = \sum_{x} b(x) \cdot \frac{\pi(x)}{b(x)} \cdot x$$

**중요도 샘플링의 몬테카를로 근사**

분포 $b$에서 $N$개의 샘플 $x^{(i)} \sim b$를 추출하고, 중요도 비율 $\rho$로 보정합니다:

$$E_{\pi}[x] \approx \frac{1}{N} \sum_{i=1}^{N} \rho^{(i)} \cdot x^{(i)}$$

여기서 중요도 비율:

$$\rho^{(i)} = \frac{\pi(x^{(i)})}{b(x^{(i)})}$$

**강화학습에서의 적용**

에피소드의 반환값 $G$에 대한 기댓값 추정:

$$V_{\pi}(s) = E_{\pi}[G] \approx \frac{1}{N} \sum_{i=1}^{N} \rho^{(i)} \cdot G^{(i)}$$

- $G^{(i)}$: 행동 정책 $b$로 수집한 $i$번째 에피소드의 반환값
- $\rho^{(i)} = \frac{\pi(a|s)}{b(a|s)}$: 해당 에피소드의 중요도 가중치


**문제 상황**: 
- **목표**: 정책 $\pi$(대상 정책)의 가치를 알고 싶음
- **현실**: 정책 $b$(행동 정책)로 수집한 데이터만 있음

**해결 방법**: 
가중치(중요도 비율) $\rho = \frac{\pi(a|s)}{b(a|s)}$를 사용해 데이터를 보정

### 중요도 샘플링 구현 및 비교

```python
import numpy as np

# --- 기본 확률 분포와 값 설정 ---
x = np.array([1, 2, 3])          # 값의 집합 (강화 학습에서는 반환값 Gt에 해당)
pi = np.array([0.1, 0.1, 0.8])   # 목표 확률 분포 (대상 정책)

# =========== Expectation (참값) ==================
# 기대값의 '참값'을 계산합니다
e = np.sum(x * pi)
print('참값 E_pi[x]:', e) 
# 결과: 참값 E_pi[x]: 2.7 

# =========== Monte Carlo (온-정책) ==================
# 목표 분포(pi)를 직접 사용하여 기대값을 추정 (온-정책 평가와 유사)
n = 100
samples = []
for _ in range(n):
    # 목표 분포 π에 따라 x에서 샘플링합니다
    s = np.random.choice(x, p=pi) 
    samples.append(s)
mean = np.mean(samples)
var = np.var(samples)

print('몬테카를로법: {:.2f} (분산: {:.2f})'.format(mean, var))
# 결과: 몬테카를로법: 2.78, (분산: 0.27)

# =========== Importance Sampling1 ===========
# 행동 분포(b)를 사용하여 목표 분포(pi)의 기대값을 추정 
b = np.array([1/3, 1/3, 1/3])    
samples = []
for _ in range(n):
    idx = np.arange(len(b))      # b의 인덱스
    i = np.random.choice(idx, p=b) 
    s = x[i]                     # 샘플링된 인덱스에 해당하는 값 s (반환값 Gt)

    # 중요도 가중치 계산: ρ = π(x) / b(x)
    rho = pi[i] / b[i] 
    
    # 가중치 적용: 가중치 ρ를 샘플 값 s에 곱하여 보정합니다
    samples.append(rho * s) 
mean = np.mean(samples)
var = np.var(samples)

print('중요도 샘플링 (균등): {:.2f} (분산: {:.2f})'.format(mean, var))
# 결과: 중요도 샘플링: 2.95, (분산: 10.63)

# =========== Importance Sampling 2: 분산을 작게하기 ===========
# 행동 분포 b를 목표 분포 pi에 가깝게 설정
b = np.array([0.2, 0.2, 0.6])    # 목표 분포와 유사한 행동 확률 분포
samples = []
for _ in range(n):
    idx = np.arange(len(b))      # [0, 1, 2] -> 샘플링할 인덱스
    
    # 1. 샘플링: 행동 분포 b를 따라 인덱스 i를 선택 (실제 환경과의 상호작용)
    i = np.random.choice(idx, p=b) 
    s = x[i]                     # 샘플링된 인덱스에 해당하는 값 s (반환값 Gt)

    # 2. 중요도 가중치 (ρ) 계산: ρ = π(x) / b(x)
    rho = pi[i] / b[i] 
    
    # 3. 가중치 적용: 가중치 ρ를 샘플 값 s에 곱하여 보정
    # 만약 목표 π가 b보다 해당 샘플을 더 자주 생성했다면 (ρ > 1), 가중치를 높입니다
    samples.append(rho * s) 

mean = np.mean(samples)
var = np.var(samples)

print('중요도 샘플링 (개선): {:.2f} (분산: {:.2f})'.format(mean, var))
# 결과: 중요도 샘플링: 2.72, (분산: 2.48)
```
**실험 결과 비교**:

| 방법 | 평균 | 분산 | 특징 |
|------|------|------|------|
| 참값 | 2.7 | - | 이론적 정확값 |
| 몬테카를로법 (온-정책) | 2.78 | 0.27 | 낮은 분산, 높은 효율성 |
| 중요도 샘플링 1 | 2.95 | 10.63 | 높은 분산 |
| 중요도 샘플링 2 (개선) | 2.72 | 2.48 | 분산 감소, 정확도 향상 |

- 가중치의 보정 효과가 클수록 분산이 커집니다
- 두 확률분포($\pi$와 $b$)를 가깝게 만들어 가중치 값을 1에 가깝게 만들면 분산을 줄일 수 있습니다
- 확률분포 $b$의 값을 $\pi$에 가깝게 만들면 평균이 2.72로 참값에 더 가까워지고 분산도 크게 줄어듭니다

---
## 5.6 정리

1. 몬테카를로법은 환경 모델 없이 에이전트의 직접 경험만으로 가치 함수를 학습하는 모델-프리 강화학습 방법입니다.

2. 정책 평가에서는 에피소드를 역순으로 순회하며 반환값($G_t$)을 계산하고, 증분 방식으로 상태 가치 함수 $V(s)$를 갱신합니다.

3. 정책 제어에서는 행동 가치 함수 $Q(s,a)$를 학습하고, $\epsilon$-Greedy 정책으로 탐색과 활용의 균형을 맞추며 최적 정책을 찾아갑니다.

4. 오프-정책은 행동 정책($b$)으로 수집한 경험을 대상 정책($\pi$)의 학습에 활용하며, 중요도 샘플링으로 두 정책 간 확률 차이를 가중치($\rho = \pi/b$)로 보정합니다.

5. 두 확률분포를 가깝게 유지하면 중요도 샘플링의 분산이 줄어들어 더 안정적이고 효율적인 학습이 가능합니다.
