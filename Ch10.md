# Chapter 10
## 10.1 심층 강화 학습 알고리즘 분류

### 1) Model Based Method

- 환경 모델 - 상태 전이 함수 $p(s'|s,a)$, 보상 함수 $r(s,a,s')$를 사용함

  - 환경모델이 주어진 경우
    - 계획만으로 문제 해결 가능
    - Dynamic Programming(DP)를 활용하여 문제 해결
    - 바둑 및 장기 같은 환경 모델이 알려져 있는 경우
    - 기법: 알파고, 알파제로
  - 환경모델이 주어지지 않은 경우
    - 직접 경험을 토대로 환경 모델을 학습
    - 계획을 수립해서 평가 및 개선(새로운 정책으로 업데이트)함
    - 기법: World Models, MBVE(Model-Based Value Estimation)
    * 환경 모델을 학습하는 경우, 샘플의 일부를 이용하기 때문에 실제 환경에서 안 좋을 수 있음

### 2) Model Free Method

- 환경 모델을 추정하거나 학습하지 않음

  - 상태전이함수 $p(s'|s,a)$확률 모델을 추정하지 않고, 그대로 관찰값을 사용하므로 TD의 부트스트랩핑도 이에 해당
  - 가치 기반 기법: DQN, SARSA 등
  - 정책 기반 기법: 기본 정책 경사법, REINFORCE, BASELINE
  - 가치 + 정책 기반 기법: 행위자-비평자(Actor Critic)

## 10.2 정책 경사법 계열의 고급 알고리즘

### 10.2.1 A3C, A2C

- A3C(Asynchronous Advantage Actor-Critic)
  - 여러 에이전트가 병렬로 행동해 비동기적 매개변수 갱신
  - 방법론
    - 신경망 모델링한 행위자-비평자를 이용해 정책 학습 진행
    - 전역 신경망과 지역 신경망을 사용
    - 지역 신경망으로부터 학습된 기울기를 활용하여 비동기적으로 가중치 매개변수 갱신
  - 장점
    - 병렬처리를 통한 학습 속도 향상
    - 개별 에이전트들의 독립적 학습을 통한 데이터 다양성 확보 (전체 상관관례 약화)
    - DQN에서는 못했던 On Policy방식에서의 상관관계 약화 전략을 가능토록 함
    - 행위자-비평자 신경망 쪽에서 $\pi(a|s), V_\pi(s)$로 분기하기 전까지 파라미터를 공유해서 학습진행
- A2C(Advantage Actor-Critic)
  - 여러 에이전트가 독립적인 $n$개 환경에서 파라미터를 동기식으로 갱신
  - 각 환경에서 time step에 따른 trajectory를 뽑아 배치를 구성해서 학습 진행
  - 신경망의 파라미터를 갱신하고, 다음 Action $A_{t+1}^{(n)}$을 각 환경에 전달 → 환경마다 액션이 다른게 샘플링됨
  - 구현하기 쉽고, GPU 자원을 효율적으로 쓸 수 있어서 A3C보다 더 많이 이용

### 10.2.2 DDPG (Deep Deterministic Policy Gradient Method)

- Action Space가 Continuous한 문제에서도 활용가능
  - 연속적인 행동 공간에서의 문제를 해결하기 위한 알고리즘
  - 정규분포의 평균을 출력하는 신경망 구성 → 정규분포에서 샘플링을 통해 Action을 획득
- 방법론
  - DDPG의 정책: Deterministic
    - 특정 state s 입력 → 그에 따른 행동 a가 고유하게 결정됨
  - 해당 정책을 DQN에 통합
    - 정책 신경망: $\mu_\theta(s)$
    - DQN Q함수 신경망: $Q_\phi(s,a)$
    - $\theta, \phi$는 각 신경망의 파라미터
  - 정책 신경망 $\mu_\theta(s)$ → Q가 커지는 행동을 출력
    - State $s$를 입력해서 연속적인 값 Action $a$ 출력
    - 두번 째 신경망인 $Q_\phi(s,a)$에 연결되어 역전파 진행 후 $\nabla_\theta q$로 $\theta$갱신
  - DQN 신경망 $Q_\phi(s,a)$
    - 현재의 입력 $s$와 정책 신경망의 출력값(연속적인 액션값) $a$를 입력해서 행동의 가치 $q$ 출력
    - DQN에서의 Q 갱신: $Q_\phi(S_t,A_t)$가 $R_t + \gamma \displaystyle\max_aQ_\phi(s,a)$에 근사시키는 것.
    - DDPG에서의 Q 갱신: 정책 신경망 $\mu_\theta(s)$의 출력을 이용해서 단순히 $Q\phi(s, \mu_\theta(s))$로 처리

### 10.2.3 TRPO, PPO

- 기존 정책 경사법의 장단점
  - 장점: 기울기를 통해 방향을 알 수 있다.
  - 단점: 얼마나 갱신해야 좋은지를 알 수 없다.
- TRPO (Trust Region Policy Optimization)
  - 신뢰영역 정책 최적화 - 적절한 갱신 폭으로 정책 최적화 가능
  - 정책 갱신 전후의 KLD를 지표 삼아 특정 임계치를 넘지 않도록 제약 → 갱신 폭이 폭발하지 않도록.
  - 헤시안 행렬을 통한 이차 미분 계산 최적화 → 계산량이 많아 병목 존재
  * Kullback Leibler Divergence(KLD): 두 확률 분포의 유사성 측정 지표
- PPO (Proximal Policy Optimization)
  - TRPO를 단순화한 기법
  - 헤시안 행렬에서의 계산량 문제를 효율화
  - 성능이 TRPO와 비슷하여 실무에서 많이 활용

## 10.3 DQN 계열의 고급 알고리즘

### 10.3.1 범주형 DQN

- 범주형 분포로 모델링하여 분포 강화 학습을 진행하는 기법
- 분포 강화학습(distribution reinforcement learning)
  - 기존의 Q함수의 기댓값 학습 → 분포를 학습시키는 방향의 아이디어
  - return의 확률분포인 $Z_\pi(s,a)$를 학습
- 범주형 분포
  - 여러 범주(이산 값) 중에 특정 범주에 속할 확률 분포
  - return $G_t$가 몇 개의 범주 영역으로 나뉘고, 그에 들어갈 확률 분포로 모델링
- 학습
  - 범주형 분포 버전의 벨만 방정식을 도출
  - 해당 방정식을 통해 범주형 분포를 갱신
  - 범주형 분포 빈(bin, 범주영역)이 51개일 때 ‘아타리’ 과제에서 성능 최고 → ‘C51’이라고도 부름

### 10.3.2 Noisy Network

- DQN의 $\varepsilon$-greedy 정책 기반 Action 결정에서 $\varepsilon$을 정하는 방법이 너무 다양한 문제를 해결

  - 신경망에 무작위성을 도입 → $\varepsilon$이 아닌 일반 greedy 정책 기반 Action 선택이 가능
  - 쉽게 표현해 출력 Layer에 노이즈 추가해서 적절한 $\varepsilon$의 효과를 추가
  - 해당 Layer의 파라미터는 정규분포의 평균, 분산으로 모델링 되며 해당 분포 내에서 샘플링 되면서 자연스럽게 노이즈가 추가됨

### 10.3.3 레인보우

- 기존의 다양한 DQN 확장 알고리즘의 결합 기법

  - 조합된 기법
    - Double DQN
    - 우선순위 경험 재생
    - Dueling DQN
    - 범주형 DQN
    - Noisy Network
  - 다른 개별 기법들에 비해 성능이 비약적으로 높음

### 10.3.4 레인보우 이후의 발전된 알고리즘

- CPU/GPU를 이용한 분산 병렬 학습(분산 강화 학습) 존재
  
  - Ape-X
    - 레인보우 기반 여러 에이전트를 CPU에서 독립적으로 Action 수행시킴
    - 에이전트 탐색(Exploration) 비중 $\varepsilon$을 모두 다르게 설정 → 다양한 경험 데이터 수집
    - 분산 병렬화를 통해 빠른 학습 + 다양한 데이터로 성능 향상
  - R2D2(Recurrent Replay Distributed DQN)
    - Ape-X를 개선하여 성능 향상
    - 순환신경망 LSTM 사용
  - NGU (Never Give Up)
    - R2D2를 발전시킨 방법
    - intrinsic reward(내적 보상) 메커니즘 추가
      - 어려운 과제, 보상이 적은 과제에도 탐색을 포기하지 않도록 처리
    - 상태 전이가 예상과 다를수록 ‘얼마나 놀랐는가’에 따른 보상 기법
  - Agent57
    - NGU를 발전시킨 기법
    - 내적 보상 메커니즘을 개선하여 ‘메타 컨트롤러’ 구조 사용 → 에이전트 별 할당 정책을 유연하게 배분
    - 아타리 2600의 전체 57개 게임에서 사람보다 우수한 성적을 거두었음

## 10.4 사례 연구

### 10.4.1 보드 게임

- 보드게임에서의 주요점
  - 바둑, 장기, 오셀로 등의 게임에서 필요한 점 - 수 읽기
  - 수 읽기 → 게임 트리 - 게임에서 발생할 수 있는 경우의 수를 노드, 엣지로 표현한 것
- MCTS(몬테카를로 트리탐색 - Monte Carlo Tree Search)
  - 트리 전개를 몬테카를로법으로 근사
  - 특정 상황에서의 보드 상태가 얼마나 좋은지 평가
  - 해당 상황에서부터 승패가 결정될 때까지 무작위로 게임 진행(플레이 아웃 또는 롤아웃)을 반복 → 승률을 통해 해당 보드의 평가
- AlphaGo(알파고)
  - MCTS에 심층 강화 학습을 결합한 기법
  - 두 개의 신경망 사용
    - 현재 보드에서 이길 확률을 평가하는 Value 신경망
    - 정책을 나타내는 Policy 신경망 → 다음에 둘 수를 확률로 출력 (e.g. (1,1)에 둘 확률 2.4%)
  - 셀프플레이
    - 에이전트 복제채인 대전 상대가 환경 역할
    - 둘의 상호작용으로 승리, 패배의 보상을 획득
    - 수집한 경험 데이터를 사용해 학습을 더욱 강화
- Alphago Zero
  - 인간의 학습 데이터를 전혀 사용하지 않고, 셀프플레이 학습만 진행
  - 도메인지식(바둑 규칙)을 이용하지 않음
    - Policy 신경망, Value 신경망을 하나로 구성
    - MCTS로 플레이아웃 하지 않고, 신경망 출력만으로 각 노드 평가
  - 기보 데이터를 활용한 알파고를 압도하는 성능
- AlphaZero
  - 알파고 제로를 미세 조정, 거의 같은 알고리즘
  - 바둑 뿐 아닌 체스, 장기까지 둘 수 있는 보드게임 종류에 상관없는 범용 알고리즘

### 10.4.2 로봇 제어

- 구글의 로봇제어 - 로봇 팔이 다양한 물건을 잡도록 학습
  - 장착된 카메라가 보내는 영상을 해석 → 행동을 결정
  - 행동 결과를 보상으로 줌
  - QT-Opt 알고리즘 활용
    - Q-Learning 기반 → OFF Policy 기법 → 과거 데이터도 활용
  - 이전에 개발한 지도 학습 방식보다 실패확률 1/5이하로 절감

### 10.4.3 NAS (Neural Architecture Search)

- 최적의 신경망 아키텍처를 자동으로 설계하는 연구 분야
  - 베이지안 최적화
  - 유전 프로그래밍
  - NASRL (Neural Architecture Search with Reinforcement Learning 논문)
- NASRL 방법
  - 신경망 아키텍처를 Text로 표현하는 점 활용
  - RNN을 통해 신경망 아키텍처를 생성하는 Action을 취함
  - 생선된 아키텍처를 검증 데이터로 학습 및 정확도 평가
  - REINFORCE 알고리즘으로 RNN 파라미터 갱신

### 10.4.4 기타 예시

- 자율주행
  - 지도학습이 Major하지만, 강화학습으로 접근하는 움직임도 보임
  - 브레이크, 핸들돌리기 등 일련의 결정이 있어 강화학습에 적합
- 건물 에너지 관리
  - 전력 소비 감소를 위한 심층 강화 학습 기법(DQN 기반)이 제안됨
- 반도체 칩 설계
  - 에이전트가 다양한 반도체 플레이아웃을 통해 최적 설계를 찾는 강화학습

## 10.5 심층 강화 학습이 풀어야 할 숙제와 가능성

### 10.5.1 현실 세계에 적용하기

- 시뮬레이터 활용
  - 위험한 Action을 수행해도 문제가 되지 않음
  - 에이전트와 환경의 상호작용 빠르게 반복 가능
  - 학습한 모델이 실제 환경에서 기대한 대로 동작하지 않을 수 있음
    - Sim2Real - Domain Randomization 기법을 통해 해결
    - 다양한 환경에서 Action 수행 및 학습을 진행
- 오프라인 강화 학습 (Offline Reinforcement Learning)
  - 과거 수집 경험 데이터를 잘 활용하는 방법
  - 환경과 상호작용을 전혀 하지 않고, 오프라인 데이터만으로 최적 정책을 추정하는 방법
- 모방 학습 (Imatation Learning)
  - 전문가의 동작을 모방하도록 정책을 학습하는 방법
    - e.g. Deep Q-Learning from Demonstrations 논문
    - 인간이 아타리를 플레이하는 모습에서 State, Action, Reward 시계열 데이터 획득
    - DQN 경험 재생 버퍼에 추가 후 학습
    - 전문가 데이터 선택될 확률 높인 뒤, 이와 가까워지도록 DQN 조정

### 10.5.2 MDP로 공식화하기 위한 팁

- MDP가 지닌 유연성
  - MDP는 환경과 에이전트가 State, Action, Reward 정보를 제공하는 구조
  - 이를 어떻게 설정할지 등 세부 정보는 유연하게 결정 가능
  - 다양한 차원의 Action, State, Time step 기준 등을 유연하게 설정할 수 있음
- MDP에서 설정이 필요한 사항
  - 현실의 문제 해결을 위한 MDP 공식화가 관건
  - 결정해야 할 사항
    - 해결하고자 하는 문제는 일회성 과지인가, 지속적 과제인가?
    - 보상의 가치는? (보상 함수 설정)
    - 에이전트가 취할 수 있는 행동은 무엇인가?
    - 환경의 상태를 무엇으로 규정할 것인가?
    - 수익의 할인율은?
    - 어디까지를 환경으로, 어디까지를 에이전트로 할 것인가?

### 10.5.3 범용 인공지능 시스템 (AGI)

- 현재 개발되는 다양한 시스템은 ‘특정 작업’에 국한됨
- 여러가지 일을 할 수 있는 범용 인공지능에 대한 연구가 진행됨
  - 딥마인드 - Reward is Enough 논문
    - “지능 혹은 기능과 관련한 능력은 보상 총합의 극대화만으로 충분히 이해할 수 있다” 가설
    - 이 말은 즉, 강화 학습으로 범용 인공지능 실현 가능성을 언급

## 10.6 정리

- Model-Based: 환경 모델을 사용/학습해 계획(planning)함
  → AlphaGo, AlphaZero
- Model-Free: 모델 없이 경험 샘플로 학습
  → DQN, REINFORCE, Actor–Critic
- A3C/A2C: Actor–Critic 병렬 버전
  - A3C: 비동기
  - A2C: 동기식(batch), GPU 효율 좋아 실무에서 더 인기
- DDPG: 연속 행동에서 쓰는 Deterministic Actor–Critic
  → Critic이 Q(s,a)로 Actor 업데이트
- TRPO/PPO: 정책 갱신 폭을 안전하게 조절
  → PPO는 실무 표준
- DQN 확장: C51(분포), NoisyNet(탐험), Rainbow(통합), Ape-X/NGU(분산)
- 응용: AlphaGo, 로봇제어, NAS 등
- 과제: Sim2Real, 오프라인 RL, MDP 설계
